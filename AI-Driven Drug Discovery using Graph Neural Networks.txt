I can provide a simplified, illustrative code example that demonstrates the core components you've described:

Molecular Data Processing (RDKit): Converting SMILES strings to molecular graphs.
Graph Data Representation (PyTorch Geometric): Storing molecular graphs in PyTorch Geometric's Data object.
Basic GNN Model (PyTorch Geometric): Defining a simple GNN architecture for molecular property prediction.
Training Loop (PyTorch): A minimal training process.
This example will focus on a molecular property prediction task, which is a fundamental component of drug discovery (e.g., predicting solubility or bioactivity).

Important Notes Before You Dive In:

Simplification: This code is a highly simplified representation. A real-world drug discovery project would involve:
Much larger and diverse datasets: (e.g., MoleculeNet benchmarks, custom datasets).
More sophisticated featurization: Beyond basic atom/bond features, including 3D information, quantum chemical properties, etc.
Complex GNN architectures: Deeper networks, attention mechanisms, pooling layers, etc.
Advanced training techniques: Learning rate schedulers, early stopping, cross-validation, hyperparameter tuning.
Robust evaluation metrics: ROC-AUC, PR-AUC, RMSE, R2, etc., depending on the task.
De Novo Generation: This is a separate, more advanced topic involving generative models (e.g., VAEs, GANs, Diffusion Models on graphs) and is not covered here.
Dependencies: You'll need to install the libraries: torch, torch_geometric, rdkit-pypi, deepchem (though we'll use RDKit and PyG directly for graph creation).
Bash

pip install torch torch_geometric rdkit-pypi deepchem
# For PyTorch Geometric, you might need to install based on your CUDA version:
# https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html
Dataset: For simplicity, we'll create a small, synthetic dataset of SMILES strings and arbitrary "target properties." In a real scenario, you'd load a dataset like ESOL, FreeSolv, or Tox21 from DeepChem or MoleculeNet.
Python

import torch
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, global_mean_pool
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset

from rdkit import Chem
from rdkit.Chem.rdmolops import Get="">
import numpy as np
import pandas as pd

# --- 1. Molecular Data Processing (RDKit) and PyTorch Geometric Data Representation ---

def one_hot_encoding(x, permitted_list):
    """
    Maps input elements x which are not in the permitted list to the last element
    of the permitted list.
    """
    if x not in permitted_list:
        x = permitted_list[-1]
    return [int(boolean_value) for boolean_value in list(map(lambda s: x == s, permitted_list))]

def get_atom_features(atom, hydrogens_implicit=True):
    """
    Generates atom features for a given RDKit atom.
    Includes atom type, formal charge, hybridization, aromaticity, and number of implicit/explicit hydrogens.
    """
    # Define a list of permitted atom types
    permitted_list_of_atoms = ['C','N','O','S','F','Si','P','Cl','Br','Mg','Na',
                               'Ca','Fe','As','Al','I','B','V','K','Tl','Yb','Sb',
                               'Sn','Ag','Pd','Co','Se','Ti','Zn','Li','Ge','Cu',
                               'Au','Ni','Cd','In','Mn','Zr','Cr','Pt','Hg','Pb','Unknown']

    atom_features = one_hot_encoding(atom.GetSymbol(), permitted_list_of_atoms)
    atom_features += one_hot_encoding(atom.GetFormalCharge(), [-2,-1,0,1,2])
    atom_features += one_hot_encoding(atom.GetHybridization().name, ['SP','SP2','SP3','SP3D','SP2D','S','UNSPECIFIED'])
    atom_features += [int(atom.GetIsAromatic())]
    if hydrogens_implicit:
        atom_features += one_hot_encoding(atom.GetTotalNumHs(), [0,1,2,3,4])
    else: # Explicit hydrogens
        atom_features += one_hot_encoding(atom.GetNumExplicitHs(), [0,1,2,3,4])
    return np.array(atom_features, dtype=np.float32)

def get_bond_features(bond):
    """
    Generates bond features for a given RDKit bond.
    Includes bond type and whether it's conjugated or in a ring.
    """
    bond_features = one_hot_encoding(bond.GetBondType().name, ['SINGLE','DOUBLE','TRIPLE','AROMATIC'])
    bond_features += [int(bond.GetIsConjugated())]
    bond_features += [int(bond.IsInRing())]
    return np.array(bond_features, dtype=np.float32)

def smiles_to_pyg_data(smiles, target_value=None):
    """
    Converts a SMILES string to a PyTorch Geometric Data object.
    """
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None

    # Get atom features (nodes)
    atom_features = [get_atom_features(atom) for atom in mol.GetAtoms()]
    x = torch.tensor(atom_features, dtype=torch.float)

    # Get bond information (edges)
    edge_indices = []
    edge_attributes = []
    for bond in mol.GetBonds():
        i = bond.GetBeginAtomIdx()
        j = bond.GetEndAtomIdx()
        bond_feat = get_bond_features(bond)
        
        # Add edges for both directions for undirected graph
        edge_indices.append([i, j])
        edge_attributes.append(bond_feat)
        edge_indices.append([j, i])
        edge_attributes.append(bond_feat)

    if not edge_indices: # Handle single atom molecules or molecules without bonds
        edge_index = torch.empty((2, 0), dtype=torch.long)
        edge_attr = torch.empty((0, len(get_bond_features(Chem.MolFromSmiles('C-C').GetBondWithIdx(0)))), dtype=torch.float) # Placeholder for empty bond features
    else:
        edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()
        edge_attr = torch.tensor(edge_attributes, dtype=torch.float)

    # Target value (if provided)
    y = None
    if target_value is not None:
        y = torch.tensor([target_value], dtype=torch.float) # Assuming a single regression target

    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=y)
    return data

# --- Create a Synthetic Dataset ---
class MolecularDataset(Dataset):
    def __init__(self, smiles_list, target_list):
        self.data_list = []
        for smiles, target in zip(smiles_list, target_list):
            data = smiles_to_pyg_data(smiles, target)
            if data is not None:
                self.data_list.append(data)

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        return self.data_list[idx]

# Example SMILES and dummy target values (e.g., predicted solubility)
synthetic_smiles = [
    "CCO",      # Ethanol
    "CCC(O)CC", # 3-Pentanol
    "C1CCCCC1", # Cyclohexane
    "c1ccccc1", # Benzene
    "O=C(O)c1ccccc1", # Benzoic Acid
    "CN1C=NC2=C1C(=O)N(C)C(=O)N2C", # Caffeine
    "CC(=O)Oc1ccccc1C(=O)O", # Aspirin
    "C(Cl)(Cl)Cl", # Chloroform
    "CCOP(=O)(OCC)S", # Malathion (insecticide, more complex)
    "NC(=O)C(=O)O", # Oxalic acid
    "O", # Water
    "C" # Methane
]
synthetic_targets = [
    1.5, 2.1, 0.8, 1.2, 0.5, 3.0, 1.8, 0.2, 0.9, 0.6, 5.0, 0.1 # Dummy solubility values
]

dataset = MolecularDataset(synthetic_smiles, synthetic_targets)
print(f"Number of samples in dataset: {len(dataset)}")
print(f"Sample 0: {dataset[0]}")
print(f"Sample 0 node features (x) shape: {dataset[0].x.shape}")
print(f"Sample 0 edge index (edge_index) shape: {dataset[0].edge_index.shape}")
print(f"Sample 0 edge attributes (edge_attr) shape: {dataset[0].edge_attr.shape}")
print(f"Sample 0 target (y): {dataset[0].y.item()}")


# --- 2. Basic GNN Model (PyTorch Geometric) ---

class GNNPredictor(torch.nn.Module):
    def __init__(self, num_node_features, num_edge_features, hidden_channels, num_classes=1):
        super(GNNPredictor, self).__init__()
        torch.manual_seed(12345)
        # GCNConv doesn't directly use edge_attr, but we include it for demonstration
        # For edge features, you might use GATConv, GINEConv, or manually integrate them.
        self.conv1 = GCNConv(num_node_features, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, hidden_channels)
        self.conv3 = GCNConv(hidden_channels, hidden_channels)
        self.lin = torch.nn.Linear(hidden_channels, num_classes) # For regression, num_classes is 1

    def forward(self, data):
        x, edge_index, batch = data.x, data.edge_index, data.batch

        # 1. Obtain node embeddings
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = self.conv3(x, edge_index)
        x = F.relu(x)

        # 2. Readout layer (pooling)
        # global_mean_pool aggregates node features into a single graph-level feature vector
        # by taking the mean of all node features belonging to each graph in the batch.
        x = global_mean_pool(x, batch)

        # 3. Apply a final linear layer for prediction
        x = self.lin(x)
        return x

# Determine feature dimensions from the first sample
if len(dataset) > 0:
    first_sample = dataset[0]
    num_node_features = first_sample.x.shape[1]
    num_edge_features = first_sample.edge_attr.shape[1] if first_sample.edge_attr.numel() > 0 else 0
else:
    raise ValueError("Dataset is empty. Cannot determine feature dimensions.")

model = GNNPredictor(num_node_features=num_node_features, num_edge_features=num_edge_features, hidden_channels=64)
print(f"\nModel architecture:\n{model}")

# --- 3. Training Loop (PyTorch) ---

# Split dataset (simple train/test split)
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)

optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
criterion = torch.nn.MSELoss() # Mean Squared Error for regression

def train():
    model.train()
    total_loss = 0
    for data in train_loader:
        optimizer.zero_grad()
        out = model(data)
        loss = criterion(out.squeeze(), data.y) # Squeeze to match dimensions for MSELoss
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * data.num_graphs
    return total_loss / len(train_dataset)

def test(loader):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for data in loader:
            out = model(data)
            loss = criterion(out.squeeze(), data.y)
            total_loss += loss.item() * data.num_graphs
    return total_loss / len(loader.dataset)

epochs = 100
print("\nStarting training...")
for epoch in range(1, epochs + 1):
    train_loss = train()
    test_loss = test(test_loader)
    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')

print("\nTraining complete.")

# --- Make a Prediction on a New Molecule ---
print("\n--- Making a prediction on a new molecule ---")
new_smiles = "CC(=O)NC1=CC=C(O)C=C1" # Acetaminophen (Paracetamol)
new_data = smiles_to_pyg_data(new_smiles, target_value=None)

if new_data:
    # Wrap in a DataLoader to simulate batching for the model
    # Even for a single molecule, PyG's DataLoader helps manage the 'batch' attribute
    new_data_loader = DataLoader([new_data], batch_size=1)
    
    model.eval() # Set model to evaluation mode
    with torch.no_grad():
        for data in new_data_loader: # Iterate once for the single molecule
            predicted_property = model(data).item()
            print(f"SMILES: {new_smiles}")
            print(f"Predicted Property (e.g., Solubility): {predicted_property:.4f}")
else:
    print(f"Could not process SMILES: {new_smiles}")
